# Decision Trees in Haskell

## Description

The implementation fully supports functionality given by the assignment and should work without problems for any valid input.

The second task which implements training of decision tree uses gini index and weighted gini index calculations to evaluate each split.

The threshold candidates are generated by creating averages of adjacent future
values from sorted array. After that, all candidates are tested to figure out which threshold is the best for the given split, based on the gini indexes, and is then used.

Tested and implemented on ghc version 9.4.8.

## Example 1: Loading and classification
```bash
./flp-fun -1 tree.txt data.txt
```

Example of the tree.txt file:
```text
Node: 0, 5.5
  Leaf: ClassA
  Node: 1, 3.0
    Leaf: ClassB
    Leaf: ClassC
```

Example of the data.txt file:
```text
2.4,1.3
6.1,0.3
6.3,4.4
```

Output:
```text
ClassA
ClassB
ClassC
```

## Example 2: Training
```bash
./flp-fun -2 train.txt
```

Example of the train.txt file:
```text
2.4,1.3,TridaA
6.1,0.3,TridaB
6.3,4.4,TridaC
2.9,4.4,TridaA
3.1,2.9,TridaB
```

Output:
```text
Node: 0, 3.0
  Leaf: TridaA
  Node: 0, 6.199999999999999
    Leaf: TridaB
    Leaf: TridaC
```
